{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cifar10.data_path = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "cifar10.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/cifar-10-batches-py/batches.meta\n"
     ]
    }
   ],
   "source": [
    "class_names = cifar10.load_class_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/cifar-10-batches-py/data_batch_5\n"
     ]
    }
   ],
   "source": [
    "images_train, cls_train, labels_train = cifar10.load_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer(\"RE_IMG_SIZE\", 24, \"resized image size\")\n",
    "#tf.app.flags.DEFINE_string()\n",
    "#tf.app.flags..DEFINE_integer(\"image_width\", 61, \"image width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distort_images(image, training):\n",
    "    if training:\n",
    "        # Randomly crop the input image.\n",
    "        distorted_image = tf.random_crop(image, size=[FLAGS.RE_IMG_SIZE, FLAGS.RE_IMG_SIZE, 3])\n",
    "        distorted_image = tf.image.random_brightness(distorted_image, max_delta=63)\n",
    "        \n",
    "        with tf.name_scope('distort_color'):\n",
    "            color_ordering = randint(0,1) # 0, 1 random number 생성\n",
    "\n",
    "            if color_ordering == 0:\n",
    "                #print('0')\n",
    "                distorted_image = tf.image.random_brightness(distorted_image, max_delta=32/255)\n",
    "                distorted_image = tf.image.random_saturation(distorted_image, lower=0.5, upper=1.5)\n",
    "                distorted_image = tf.image.random_hue(distorted_image, max_delta=0.05)\n",
    "                distorted_image = tf.image.random_contrast(distorted_image, lower=0.5, upper=1.5)\n",
    "            else:\n",
    "                #print('1')\n",
    "                distorted_image = tf.image.random_brightness(distorted_image, max_delta=32/255)\n",
    "                distorted_image = tf.image.random_contrast(distorted_image, lower=0.5, upper=1.5)\n",
    "                distorted_image = tf.image.random_saturation(distorted_image, lower=0.5, upper=1.5)\n",
    "                distorted_image = tf.image.random_hue(distorted_image, max_delta=0.05)\n",
    "            \n",
    "            distorted_image = tf.clip_by_value(distorted_image, 0.0, 1.0)\n",
    "            \n",
    "    else:\n",
    "        distorted_image = tf.image.resize_image_with_crop_or_pad(image,\n",
    "                                                              target_height = FLAGS.RE_IMG_SIZE,\n",
    "                                                              target_width = FLAGS.RE_IMG_SIZE)\n",
    "        \n",
    "        distorted_image = tf.clip_by_value(distorted_image, 0.0, 1.0)\n",
    "        distorted_image = tf.cast(distorted_image, tf.float32)\n",
    "    \n",
    "    return distorted_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process(images, training):\n",
    "    images = tf.map_fn(lambda image: distort_images(image, training), images)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(x, y_, training):\n",
    "    \n",
    "    # Convolution Layer1\n",
    "    with tf.variable_scope('conv_block1'):\n",
    "        conv1 = tf.contrib.layers.convolution2d(inputs=x, num_outputs=64,\n",
    "                                        kernel_size=5,\n",
    "                                        stride=1,\n",
    "                                        padding='SAME',\n",
    "                                        activation_fn=None,\n",
    "                                        weights_initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                        biases_initializer=None,\n",
    "                                        scope='conv1'\n",
    "                                           )\n",
    "\n",
    "        pool_1 = tf.contrib.layers.max_pool2d(inputs=conv1,\n",
    "                                              kernel_size=3,\n",
    "                                              stride=2,\n",
    "                                              padding='SAME',\n",
    "                                              scope='pool_1')\n",
    "\n",
    "        batch_1 = tf.contrib.layers.batch_norm(pool_1, \n",
    "                                               center=True,\n",
    "                                               scale=True,\n",
    "                                               is_training=training,\n",
    "                                               scope='bn')\n",
    "        \n",
    "        act1 = tf.nn.relu(batch_1, name='activation_1')\n",
    "    \n",
    "    # Convolution Layer2\n",
    "    with tf.variable_scope('conv_block2'):\n",
    "        conv2 = tf.contrib.layers.convolution2d(inputs=act1, num_outputs=64,\n",
    "                                        kernel_size=5,\n",
    "                                        stride=1,\n",
    "                                        padding='SAME',\n",
    "                                        activation_fn=None,\n",
    "                                        weights_initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                        biases_initializer=None\n",
    "                                        )\n",
    "\n",
    "        pool_2 = tf.contrib.layers.max_pool2d(inputs=conv2,\n",
    "                                              kernel_size=3,\n",
    "                                              stride=2,\n",
    "                                              padding='SAME')\n",
    "\n",
    "        batch_2 = tf.contrib.layers.batch_norm(pool_2,\n",
    "                                               center=True,\n",
    "                                               scale=True,\n",
    "                                               is_training=training,\n",
    "                                               scope='bn')\n",
    "        \n",
    "        act2 = tf.nn.relu(batch_1, name='activation_2')\n",
    "    \n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    flatten = tf.contrib.layers.flatten(act2, scope='flatten')\n",
    "    \n",
    "    with tf.variable_scope('fc_layer'):\n",
    "        fc1 = tf.contrib.layers.fully_connected(flatten,num_outputs=384,\n",
    "                                            activation_fn=tf.nn.relu,\n",
    "                                            weights_initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                            biases_initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "\n",
    "        fc2 = tf.contrib.layers.fully_connected(fc1,num_outputs=192,\n",
    "                                            activation_fn=tf.nn.relu,\n",
    "                                            weights_initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                            biases_initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "    \n",
    "    # Logits(final Layer)\n",
    "    with tf.variable_scope('Logits'):\n",
    "        logits = tf.contrib.layers.fully_connected(fc2,num_outputs=10,\n",
    "                                            activation_fn=None,\n",
    "                                            weights_initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                            biases_initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "    \n",
    "    \n",
    "    \n",
    "    one_hot = tf.one_hot(y_, 10, name='one_hot')\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot)\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "    \n",
    "    # Evaluate Pipeline\n",
    "    correct_prediction = tf.equal(tf.arg_max(logits, 1), tf.argmax(one_hot, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    \n",
    "    return logits, loss, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(images, labels, epochs, batch_size):\n",
    "    \n",
    "    train_size = len(images) #50000\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, FLAGS.RE_IMG_SIZE, FLAGS.RE_IMG_SIZE, 3], name='x')\n",
    "    y = tf.placeholder(tf.int32, [None], name='y')\n",
    "    \n",
    "    logits, loss, accuracy = inference(x, y, True)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('ADAM'):\n",
    "            batch_step = tf.Variable(initial_value=0,\n",
    "                                      name='global_step',\n",
    "                                      trainable=False)\n",
    "            \n",
    "            learning_rate = tf.train.exponential_decay(1e-4,\n",
    "                                                       global_step= batch_step* batch_size,\n",
    "                                                       decay_steps= train_size,\n",
    "                                                       decay_rate=0.95,\n",
    "                                                       staircase=True\n",
    "                                                      )\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=batch_step)\n",
    "    \n",
    "    \n",
    "    # Keep track of learning rate\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "        summary_writer = tf.summary.FileWriter('./log', sess.graph)\n",
    "        \n",
    "        batch_step = 0\n",
    "        for e in range(epochs):\n",
    "            \n",
    "            # Shake input datas\n",
    "            x_train, y_train = shuffle(images, labels)\n",
    "            \n",
    "            for offset in range(0, train_size, batch_size):\n",
    "                batch_step +=1\n",
    "                print('batch_step', batch_step)\n",
    "                end = offset + batch_size\n",
    "                x_batch, y_batch = x_train[offset:end], y_train[offset:end]\n",
    "\n",
    "                # distort images, randomly\n",
    "                x_batch = pre_process(x_batch, training=True)\n",
    "                x_batch = sess.run(x_batch)\n",
    "                \n",
    "                feed_train = {x: x_batch, y: y_batch}\n",
    "\n",
    "                # parameter update(training)\n",
    "                sess.run(train_op, feed_dict = feed_train)\n",
    "                #train_loss = sess.run(loss, feed_dict = feed_train)\n",
    "                train_accuracy = sess.run(accuracy, feed_dict = feed_train)\n",
    "                \n",
    "                \n",
    "                # print()\n",
    "                if(batch_step % 10 == 0):\n",
    "                    print('Epoch:{0}, batch_step:{1}, train_accuracy,{2}'.format(e, batch_step, train_accuracy))\n",
    "                \n",
    "                # Update the events file.\n",
    "                summary_str = sess.run(summary, feed_dict=feed_train)\n",
    "                summary_writer.add_summary(summary_str, batch_step)\n",
    "                summary_writer.flush()\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            saver\n",
    "        except NameError:\n",
    "            saver = tf.train.Saver()\n",
    "        saver.save(sess, 'cifar10')\n",
    "        print(\"Model saved\")\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_step 1\n",
      "batch_step 2\n",
      "batch_step 3\n",
      "batch_step 4\n",
      "batch_step 5\n",
      "batch_step 6\n",
      "batch_step 7\n",
      "batch_step 8\n",
      "batch_step 9\n",
      "batch_step 10\n",
      "Epoch:0, batch_step:10, train_accuracy,0.1171875\n",
      "batch_step 11\n",
      "batch_step 12\n",
      "batch_step 13\n",
      "batch_step 14\n",
      "batch_step 15\n",
      "batch_step 16\n",
      "batch_step 17\n",
      "batch_step 18\n",
      "batch_step 19\n",
      "batch_step 20\n",
      "Epoch:0, batch_step:20, train_accuracy,0.09375\n",
      "batch_step 21\n",
      "batch_step 22\n",
      "batch_step 23\n",
      "batch_step 24\n",
      "batch_step 25\n",
      "batch_step 26\n",
      "batch_step 27\n",
      "batch_step 28\n",
      "batch_step 29\n",
      "batch_step 30\n",
      "Epoch:0, batch_step:30, train_accuracy,0.0546875\n",
      "batch_step 31\n",
      "batch_step 32\n",
      "batch_step 33\n",
      "batch_step 34\n",
      "batch_step 35\n",
      "batch_step 36\n",
      "batch_step 37\n",
      "batch_step 38\n",
      "batch_step 39\n",
      "batch_step 40\n",
      "Epoch:0, batch_step:40, train_accuracy,0.078125\n",
      "batch_step 41\n",
      "batch_step 42\n",
      "batch_step 43\n",
      "batch_step 44\n",
      "batch_step 45\n",
      "batch_step 46\n",
      "batch_step 47\n",
      "batch_step 48\n",
      "batch_step 49\n",
      "batch_step 50\n",
      "Epoch:0, batch_step:50, train_accuracy,0.1328125\n",
      "batch_step 51\n",
      "batch_step 52\n",
      "batch_step 53\n",
      "batch_step 54\n",
      "batch_step 55\n",
      "batch_step 56\n",
      "batch_step 57\n",
      "batch_step 58\n",
      "batch_step 59\n",
      "batch_step 60\n",
      "Epoch:0, batch_step:60, train_accuracy,0.1015625\n",
      "batch_step 61\n",
      "batch_step 62\n",
      "batch_step 63\n",
      "batch_step 64\n",
      "batch_step 65\n",
      "batch_step 66\n",
      "batch_step 67\n",
      "batch_step 68\n",
      "batch_step 69\n",
      "batch_step 70\n",
      "Epoch:0, batch_step:70, train_accuracy,0.0625\n",
      "batch_step 71\n",
      "batch_step 72\n",
      "batch_step 73\n",
      "batch_step 74\n",
      "batch_step 75\n",
      "batch_step 76\n",
      "batch_step 77\n",
      "batch_step 78\n",
      "batch_step 79\n",
      "batch_step 80\n",
      "Epoch:0, batch_step:80, train_accuracy,0.078125\n",
      "batch_step 81\n",
      "batch_step 82\n",
      "batch_step 83\n",
      "batch_step 84\n",
      "batch_step 85\n",
      "batch_step 86\n",
      "batch_step 87\n",
      "batch_step 88\n",
      "batch_step 89\n",
      "batch_step 90\n",
      "Epoch:0, batch_step:90, train_accuracy,0.15625\n",
      "batch_step 91\n",
      "batch_step 92\n",
      "batch_step 93\n",
      "batch_step 94\n",
      "batch_step 95\n",
      "batch_step 96\n",
      "batch_step 97\n",
      "batch_step 98\n",
      "batch_step 99\n",
      "batch_step 100\n",
      "Epoch:0, batch_step:100, train_accuracy,0.125\n",
      "batch_step 101\n",
      "batch_step 102\n",
      "batch_step 103\n",
      "batch_step 104\n",
      "batch_step 105\n",
      "batch_step 106\n",
      "batch_step 107\n",
      "batch_step 108\n",
      "batch_step 109\n",
      "batch_step 110\n",
      "Epoch:0, batch_step:110, train_accuracy,0.1171875\n",
      "batch_step 111\n",
      "batch_step 112\n",
      "batch_step 113\n",
      "batch_step 114\n",
      "batch_step 115\n",
      "batch_step 116\n",
      "batch_step 117\n",
      "batch_step 118\n",
      "batch_step 119\n",
      "batch_step 120\n",
      "Epoch:0, batch_step:120, train_accuracy,0.1484375\n",
      "batch_step 121\n",
      "batch_step 122\n",
      "batch_step 123\n",
      "batch_step 124\n",
      "batch_step 125\n",
      "batch_step 126\n",
      "batch_step 127\n",
      "batch_step 128\n",
      "batch_step 129\n",
      "batch_step 130\n",
      "Epoch:0, batch_step:130, train_accuracy,0.0859375\n",
      "batch_step 131\n",
      "batch_step 132\n",
      "batch_step 133\n",
      "batch_step 134\n",
      "batch_step 135\n",
      "batch_step 136\n",
      "batch_step 137\n",
      "batch_step 138\n",
      "batch_step 139\n",
      "batch_step 140\n",
      "Epoch:0, batch_step:140, train_accuracy,0.1015625\n",
      "batch_step 141\n",
      "batch_step 142\n",
      "batch_step 143\n",
      "batch_step 144\n",
      "batch_step 145\n",
      "batch_step 146\n",
      "batch_step 147\n",
      "batch_step 148\n",
      "batch_step 149\n",
      "batch_step 150\n",
      "Epoch:0, batch_step:150, train_accuracy,0.109375\n",
      "batch_step 151\n",
      "batch_step 152\n",
      "batch_step 153\n",
      "batch_step 154\n",
      "batch_step 155\n",
      "batch_step 156\n",
      "batch_step 157\n",
      "batch_step 158\n",
      "batch_step 159\n",
      "batch_step 160\n",
      "Epoch:0, batch_step:160, train_accuracy,0.1640625\n",
      "batch_step 161\n",
      "batch_step 162\n",
      "batch_step 163\n",
      "batch_step 164\n",
      "batch_step 165\n",
      "batch_step 166\n",
      "batch_step 167\n",
      "batch_step 168\n",
      "batch_step 169\n",
      "batch_step 170\n",
      "Epoch:0, batch_step:170, train_accuracy,0.109375\n",
      "batch_step 171\n",
      "batch_step 172\n",
      "batch_step 173\n",
      "batch_step 174\n",
      "batch_step 175\n",
      "batch_step 176\n",
      "batch_step 177\n",
      "batch_step 178\n",
      "batch_step 179\n",
      "batch_step 180\n",
      "Epoch:0, batch_step:180, train_accuracy,0.078125\n",
      "batch_step 181\n",
      "batch_step 182\n",
      "batch_step 183\n",
      "batch_step 184\n",
      "batch_step 185\n",
      "batch_step 186\n",
      "batch_step 187\n",
      "batch_step 188\n",
      "batch_step 189\n",
      "batch_step 190\n",
      "Epoch:0, batch_step:190, train_accuracy,0.0859375\n",
      "batch_step 191\n",
      "batch_step 192\n",
      "batch_step 193\n",
      "batch_step 194\n",
      "batch_step 195\n",
      "batch_step 196\n",
      "batch_step 197\n",
      "batch_step 198\n",
      "batch_step 199\n",
      "batch_step 200\n",
      "Epoch:0, batch_step:200, train_accuracy,0.1015625\n",
      "batch_step 201\n",
      "batch_step 202\n",
      "batch_step 203\n",
      "batch_step 204\n",
      "batch_step 205\n",
      "batch_step 206\n",
      "batch_step 207\n",
      "batch_step 208\n",
      "batch_step 209\n",
      "batch_step 210\n",
      "Epoch:0, batch_step:210, train_accuracy,0.0859375\n",
      "batch_step 211\n",
      "batch_step 212\n",
      "batch_step 213\n",
      "batch_step 214\n",
      "batch_step 215\n",
      "batch_step 216\n",
      "batch_step 217\n",
      "batch_step 218\n",
      "batch_step 219\n",
      "batch_step 220\n",
      "Epoch:0, batch_step:220, train_accuracy,0.09375\n",
      "batch_step 221\n",
      "batch_step 222\n",
      "batch_step 223\n",
      "batch_step 224\n",
      "batch_step 225\n",
      "batch_step 226\n",
      "batch_step 227\n",
      "batch_step 228\n",
      "batch_step 229\n",
      "batch_step 230\n",
      "Epoch:0, batch_step:230, train_accuracy,0.0625\n",
      "batch_step 231\n",
      "batch_step 232\n",
      "batch_step 233\n",
      "batch_step 234\n",
      "batch_step 235\n",
      "batch_step 236\n",
      "batch_step 237\n",
      "batch_step 238\n",
      "batch_step 239\n",
      "batch_step 240\n",
      "Epoch:0, batch_step:240, train_accuracy,0.1171875\n",
      "batch_step 241\n",
      "batch_step 242\n",
      "batch_step 243\n",
      "batch_step 244\n",
      "batch_step 245\n",
      "batch_step 246\n",
      "batch_step 247\n",
      "batch_step 248\n",
      "batch_step 249\n",
      "batch_step 250\n",
      "Epoch:0, batch_step:250, train_accuracy,0.125\n",
      "batch_step 251\n",
      "batch_step 252\n",
      "batch_step 253\n",
      "batch_step 254\n",
      "batch_step 255\n",
      "batch_step 256\n",
      "batch_step 257\n",
      "batch_step 258\n",
      "batch_step 259\n",
      "batch_step 260\n",
      "Epoch:0, batch_step:260, train_accuracy,0.1484375\n",
      "batch_step 261\n",
      "batch_step 262\n",
      "batch_step 263\n",
      "batch_step 264\n",
      "batch_step 265\n",
      "batch_step 266\n",
      "batch_step 267\n",
      "batch_step 268\n",
      "batch_step 269\n",
      "batch_step 270\n",
      "Epoch:0, batch_step:270, train_accuracy,0.09375\n",
      "batch_step 271\n",
      "batch_step 272\n",
      "batch_step 273\n",
      "batch_step 274\n",
      "batch_step 275\n",
      "batch_step 276\n",
      "batch_step 277\n",
      "batch_step 278\n",
      "batch_step 279\n",
      "batch_step 280\n",
      "Epoch:0, batch_step:280, train_accuracy,0.1875\n",
      "batch_step 281\n",
      "batch_step 282\n",
      "batch_step 283\n",
      "batch_step 284\n",
      "batch_step 285\n",
      "batch_step 286\n",
      "batch_step 287\n",
      "batch_step 288\n",
      "batch_step 289\n",
      "batch_step 290\n",
      "Epoch:0, batch_step:290, train_accuracy,0.0625\n",
      "batch_step 291\n",
      "batch_step 292\n",
      "batch_step 293\n",
      "batch_step 294\n",
      "batch_step 295\n",
      "batch_step 296\n",
      "batch_step 297\n",
      "batch_step 298\n",
      "batch_step 299\n",
      "batch_step 300\n",
      "Epoch:0, batch_step:300, train_accuracy,0.109375\n",
      "batch_step 301\n",
      "batch_step 302\n",
      "batch_step 303\n",
      "batch_step 304\n",
      "batch_step 305\n",
      "batch_step 306\n",
      "batch_step 307\n",
      "batch_step 308\n",
      "batch_step 309\n",
      "batch_step 310\n",
      "Epoch:0, batch_step:310, train_accuracy,0.1171875\n",
      "batch_step 311\n",
      "batch_step 312\n",
      "batch_step 313\n",
      "batch_step 314\n",
      "batch_step 315\n",
      "batch_step 316\n",
      "batch_step 317\n",
      "batch_step 318\n",
      "batch_step 319\n",
      "batch_step 320\n",
      "Epoch:0, batch_step:320, train_accuracy,0.140625\n",
      "batch_step 321\n",
      "batch_step 322\n",
      "batch_step 323\n",
      "batch_step 324\n",
      "batch_step 325\n",
      "batch_step 326\n",
      "batch_step 327\n",
      "batch_step 328\n",
      "batch_step 329\n",
      "batch_step 330\n",
      "Epoch:0, batch_step:330, train_accuracy,0.09375\n",
      "batch_step 331\n",
      "batch_step 332\n",
      "batch_step 333\n",
      "batch_step 334\n",
      "batch_step 335\n",
      "batch_step 336\n",
      "batch_step 337\n",
      "batch_step 338\n",
      "batch_step 339\n",
      "batch_step 340\n",
      "Epoch:0, batch_step:340, train_accuracy,0.1015625\n",
      "batch_step 341\n",
      "batch_step 342\n",
      "batch_step 343\n",
      "batch_step 344\n",
      "batch_step 345\n",
      "batch_step 346\n",
      "batch_step 347\n",
      "batch_step 348\n",
      "batch_step 349\n",
      "batch_step 350\n",
      "Epoch:0, batch_step:350, train_accuracy,0.09375\n",
      "batch_step 351\n",
      "batch_step 352\n",
      "batch_step 353\n",
      "batch_step 354\n",
      "batch_step 355\n",
      "batch_step 356\n",
      "batch_step 357\n",
      "batch_step 358\n",
      "batch_step 359\n",
      "batch_step 360\n",
      "Epoch:0, batch_step:360, train_accuracy,0.0546875\n",
      "batch_step 361\n",
      "batch_step 362\n",
      "batch_step 363\n",
      "batch_step 364\n",
      "batch_step 365\n",
      "batch_step 366\n",
      "batch_step 367\n",
      "batch_step 368\n",
      "batch_step 369\n",
      "batch_step 370\n",
      "Epoch:0, batch_step:370, train_accuracy,0.078125\n",
      "batch_step 371\n",
      "batch_step 372\n",
      "batch_step 373\n",
      "batch_step 374\n",
      "batch_step 375\n",
      "batch_step 376\n",
      "batch_step 377\n",
      "batch_step 378\n",
      "batch_step 379\n",
      "batch_step 380\n",
      "Epoch:0, batch_step:380, train_accuracy,0.0625\n",
      "batch_step 381\n",
      "batch_step 382\n",
      "batch_step 383\n",
      "batch_step 384\n",
      "batch_step 385\n",
      "batch_step 386\n",
      "batch_step 387\n",
      "batch_step 388\n",
      "batch_step 389\n",
      "batch_step 390\n",
      "Epoch:0, batch_step:390, train_accuracy,0.09375\n",
      "batch_step 391\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train(images=images_train, labels=cls_train, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/cifar-10-batches-py/test_batch\n"
     ]
    }
   ],
   "source": [
    "images_test, cls_test, labels_test = cifar10.load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check1\n",
      "check2\n",
      "check3\n",
      "check4\n",
      "check5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "TensorArray dtype is double but Op is trying to write dtype float.\n\t [[Node: map/while/TensorArrayWriteV2_1 = TensorArrayWriteV2[T=DT_FLOAT, _class=[\"loc:@map/TensorArray_3\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](map/while/TensorArrayWriteV2_1/Enter, map/while/Identity_2, map/while/Cast, map/while/Identity_3)]]\n\nCaused by op 'map/while/TensorArrayWriteV2_1', defined at:\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-cd23df18a7dd>\", line 23, in <module>\n    batch_x = pre_process(batch_x, training=False)\n  File \"<ipython-input-12-0a0b07e766ef>\", line 2, in pre_process\n    images = tf.map_fn(lambda image: distort_images(image, training), images)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py\", line 390, in map_fn\n    swap_memory=swap_memory)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2636, in while_loop\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2469, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2419, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py\", line 383, in compute\n    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py\", line 383, in <listcomp>\n    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 216, in write\n    name=name)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 2076, in _tensor_array_write_v2\n    name=name)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): TensorArray dtype is double but Op is trying to write dtype float.\n\t [[Node: map/while/TensorArrayWriteV2_1 = TensorArrayWriteV2[T=DT_FLOAT, _class=[\"loc:@map/TensorArray_3\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](map/while/TensorArrayWriteV2_1/Enter, map/while/Identity_2, map/while/Cast, map/while/Identity_3)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: TensorArray dtype is double but Op is trying to write dtype float.\n\t [[Node: map/while/TensorArrayWriteV2_1 = TensorArrayWriteV2[T=DT_FLOAT, _class=[\"loc:@map/TensorArray_3\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](map/while/TensorArrayWriteV2_1/Enter, map/while/Identity_2, map/while/Cast, map/while/Identity_3)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-cd23df18a7dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# distort images, randomly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"check6\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0maccuracy_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: TensorArray dtype is double but Op is trying to write dtype float.\n\t [[Node: map/while/TensorArrayWriteV2_1 = TensorArrayWriteV2[T=DT_FLOAT, _class=[\"loc:@map/TensorArray_3\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](map/while/TensorArrayWriteV2_1/Enter, map/while/Identity_2, map/while/Cast, map/while/Identity_3)]]\n\nCaused by op 'map/while/TensorArrayWriteV2_1', defined at:\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-cd23df18a7dd>\", line 23, in <module>\n    batch_x = pre_process(batch_x, training=False)\n  File \"<ipython-input-12-0a0b07e766ef>\", line 2, in pre_process\n    images = tf.map_fn(lambda image: distort_images(image, training), images)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py\", line 390, in map_fn\n    swap_memory=swap_memory)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2636, in while_loop\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2469, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2419, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py\", line 383, in compute\n    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py\", line 383, in <listcomp>\n    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 216, in write\n    name=name)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 2076, in _tensor_array_write_v2\n    name=name)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/jihobak/.pyenv/versions/miniconda3-4.0.5/envs/CarND-LeNet-Lab/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): TensorArray dtype is double but Op is trying to write dtype float.\n\t [[Node: map/while/TensorArrayWriteV2_1 = TensorArrayWriteV2[T=DT_FLOAT, _class=[\"loc:@map/TensorArray_3\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](map/while/TensorArrayWriteV2_1/Enter, map/while/Identity_2, map/while/Cast, map/while/Identity_3)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # x, y, logits, train, accuracy = build_graph(model_config)\n",
    "    print(\"check1\")\n",
    "    x = tf.placeholder(tf.float32, [None, FLAGS.RE_IMG_SIZE, FLAGS.RE_IMG_SIZE, 3], name='x')\n",
    "    y = tf.placeholder(tf.int32, [None], name='y')\n",
    "    \n",
    "    logits, loss, accuracy = inference(x, y, False)\n",
    "    print(\"check2\")\n",
    "    loader = tf.train.import_meta_graph('cifar10.meta')\n",
    "    loader.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    print(\"check3\")\n",
    "    batch_size = 128\n",
    "    valid_size = len(images_test)\n",
    "    \n",
    "    # to get session using in training.\n",
    "    sess = tf.get_default_session()\n",
    "    total_accuracy = 0\n",
    "    print(\"check4\")\n",
    "    for offset in range(0, valid_size, batch_size):\n",
    "        batch_x, batch_y = images_test[offset:offset+batch_size], cls_test[offset:offset+batch_size]\n",
    "        print(\"check5\")\n",
    "        # distort images, randomly\n",
    "        batch_x = pre_process(batch_x, training=False)\n",
    "        batch_x = sess.run(batch_x)\n",
    "        print(\"check6\")\n",
    "        accuracy_value = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "        print(\">>>>>>>>>>>>>>\", accuracy_value)\n",
    "        total_accuracy += (accuracy_value * batch_size)\n",
    "    \n",
    "    test_accuracy = total_accuracy / valid_size\n",
    "    \n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:CarND-LeNet-Lab]",
   "language": "python",
   "name": "conda-env-CarND-LeNet-Lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
